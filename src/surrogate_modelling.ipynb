{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import kthread\n",
    "import threading\n",
    "import env_vars\n",
    "import csv\n",
    "import random\n",
    "import ast\n",
    "import helpers\n",
    "import torch as T\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset \n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from sklearn import preprocessing\n",
    "import copy\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = None\n",
    "\n",
    "# configs\n",
    "\n",
    "with open(os.path.join(env_vars.PROJ_HOME, 'src/VSM/parser/output_copter.csv')) as csv_file:\n",
    "    configs = {}\n",
    "    file_data = csv.reader(csv_file, delimiter=',')\n",
    "    headers = next(file_data)\n",
    "    for row in file_data:\n",
    "        row_dict = dict(zip(headers, row))\n",
    "\n",
    "        configs[row[0]] = {\n",
    "            'name': row_dict['Name'],\n",
    "            'description': row_dict['Description'],\n",
    "            'range': [] if row_dict['Range'] == '' else  row_dict['Range'].split(' '),\n",
    "            'increment': None if row_dict['Increment'] == '' else float(row_dict['Increment']), \n",
    "            'value': [] if row_dict['Value'] == '' else  [v for v in row_dict['Value'].split('|') if v != '']\n",
    "        }\n",
    "\n",
    "rt1_5p_dir = '/home/anon/Documents/dronefuzzingresearch/results/5_params/A.RTL1/rand/'\n",
    "rt1_10p_dir = '/home/anon/Documents/dronefuzzingresearch/results/10_params/A.RTL1/rand/'\n",
    "rt1_15p_dir = '/home/anon/Documents/dronefuzzingresearch/results/15_params/A.RTL1/rand/'\n",
    "rt1_20p_dir = '/home/anon/Documents/dronefuzzingresearch/results/20_params/A.RTL1/rand/'\n",
    "rt2_5p_dir = '/home/anon/Documents/dronefuzzingresearch/results/5_params/A.RTL2/rand/'\n",
    "rt2_10p_dir = '/home/anon/Documents/dronefuzzingresearch/results/10_params/A.RTL2/rand/'\n",
    "rt2_15p_dir = '/home/anon/Documents/dronefuzzingresearch/results/15_params/A.RTL2/rand/'\n",
    "rt2_20p_dir = '/home/anon/Documents/dronefuzzingresearch/results/20_params/A.RTL2/rand/'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_log(field, value):\n",
    "    if field in env_vars.state_ranges:\n",
    "        min_val = float(env_vars.state_ranges[field]['min'])\n",
    "        max_val = float(env_vars.state_ranges[field]['max'])\n",
    "        return (value - min_val) / (max_val - min_val)\n",
    "    else:\n",
    "        return value\n",
    "    \n",
    "def normalize_param(param, value):\n",
    "    min_val = float(configs[param]['range'][0])\n",
    "    max_val = float(configs[param]['range'][1])\n",
    "    # print('param', min_val, max_val)\n",
    "    return ((value - min_val) / (max_val - min_val))\n",
    "\n",
    "def one_hot_column(row, target_mode):\n",
    "    if row['MODE'] == target_mode:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def extract_logs(dir, target_scenario):\n",
    "    rand_iterations_dir = [d for d in os.listdir(dir) if os.path.isdir(dir + d)]\n",
    "    sorted_rand_iterations = [d for d in sorted(rand_iterations_dir)]\n",
    "\n",
    "    for iteration in sorted_rand_iterations:\n",
    "        log_df = pd.read_csv(dir  + iteration + '/' + 'streamed_logs_' + iteration + '.csv', index_col=0)\n",
    "        log_df = log_df[env_vars.simulation_vars[target_scenario]['state_cols']+['MODE']]\n",
    "\n",
    "        # one-hot encode the mode (each mode has a column)\n",
    "        for mode in env_vars.simulation_vars[target_scenario]['modes_vec']:\n",
    "            # add column\n",
    "            log_df[\"MODE_\"+mode] = log_df.apply(lambda row: one_hot_column(row, mode), axis=1)\n",
    "        log_df = log_df.drop(columns=['MODE'])\n",
    "\n",
    "        log_df[sorted(log_df.columns, key=lambda x: x.lower())].to_csv(dir  + iteration + '/' + 'sorted_streamed_logs_' + iteration + '.csv')\n",
    "\n",
    "        # params already sorted by top_k\n",
    "        # param_df = pd.read_csv(dir  + iteration + '/' + 'params_records_' + iteration + '.csv', index_col=0)\n",
    "        # param_df[sorted(param_df.columns)].to_csv(dir  + iteration + '/' + 'sorted_params_records_' + iteration + '.csv')\n",
    "        # param_df.to_csv(dir  + iteration + '/' + 'sorted_params_records_' + iteration + '.csv')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract_logs(rt1_5p_dir, 'A.RTL1')\n",
    "# extract_logs(rt1_10p_dir, 'A.RTL1')\n",
    "# extract_logs(rt1_15p_dir, 'A.RTL1')\n",
    "# extract_logs(rt1_20p_dir, 'A.RTL1')\n",
    "# extract_logs(rt2_5p_dir, 'A.RTL2')\n",
    "# extract_logs(rt2_10p_dir, 'A.RTL2')\n",
    "extract_logs(rt2_20p_dir, 'A.RTL2')\n",
    "# extract_logs(rt2_15p_dir, 'A.RTL2')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mine Extracted Logs for Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mine_datapoints(dir, scenario):\n",
    "\n",
    "    iteration_state_params = {}\n",
    "    rand_iterations_dir = [d for d in os.listdir(dir) if os.path.isdir(dir + d)]\n",
    "    sorted_rand_iterations = [d for d in sorted(rand_iterations_dir)]\n",
    "\n",
    "    for iteration in sorted_rand_iterations:\n",
    "        \n",
    "        vio_df = pd.read_csv(dir  + iteration + '/' + 'violation_scores_' + iteration + '.csv', index_col=0)\n",
    "        params_df = pd.read_csv(dir  + iteration + '/' + 'params_records_' + iteration + '.csv', index_col=0)\n",
    "        log_df = pd.read_csv(dir  + iteration + '/' + 'sorted_streamed_logs_' + iteration + '.csv', index_col='time_stamp')\n",
    "\n",
    "        iteration_state_params[iteration] = []\n",
    "\n",
    "        log_pos = 0\n",
    "        vio_pos = 0\n",
    "\n",
    "        try:\n",
    "\n",
    "            for param_ts in params_df.index:\n",
    "\n",
    "                vio_score = 0\n",
    "\n",
    "                while True:\n",
    "                    if vio_pos >= len(vio_df.index):\n",
    "                        break\n",
    "                    curr_vio_time = vio_df.index[vio_pos]\n",
    "                    if datetime.strptime(curr_vio_time, '%H:%M:%S.%f') > datetime.strptime(param_ts, '%H:%M:%S.%f'):\n",
    "                        \n",
    "                        # vio_score = vio_df.loc\n",
    "                        rows = vio_df[vio_pos: vio_pos+15]\n",
    "                        violations = rows[scenario].tolist()\n",
    "                        vio_score = max(violations)\n",
    "\n",
    "                        break\n",
    "                    else:\n",
    "                        vio_pos += 1\n",
    "\n",
    "                while True:\n",
    "                    if log_pos >= len(log_df.index):\n",
    "                        break\n",
    "                    curr_log_time = log_df.index[log_pos]\n",
    "                    if datetime.strptime(curr_log_time, '%H:%M:%S.%f') > datetime.strptime(param_ts, '%H:%M:%S.%f'):\n",
    "                        if log_pos < 5:\n",
    "                            break\n",
    "\n",
    "                        new_data_point = {\n",
    "                            'param_set_time': param_ts,\n",
    "                            'params_set': params_df.loc[param_ts].to_dict(),\n",
    "                            'this_state': log_df.iloc[log_pos].to_dict(),\n",
    "                            'next_state': log_df.iloc[log_pos+1].to_dict(),\n",
    "                            'violation': vio_score\n",
    "                        }\n",
    "\n",
    "                        normalized_data_point = {\n",
    "                            'param_set_time': param_ts,\n",
    "                            'params_set': [normalize_param(p, new_data_point['params_set'][p]) for p in new_data_point['params_set']],\n",
    "                            'this_state': [normalize_log(l, new_data_point['this_state'][l]) for l in new_data_point['this_state']],\n",
    "                            'next_state': [normalize_log(l, new_data_point['next_state'][l]) for l in new_data_point['next_state']],\n",
    "                            'violation': vio_score\n",
    "                        }\n",
    "                                \n",
    "                        # iteration_state_params[iteration].append(new_data_point)\n",
    "                        iteration_state_params[iteration].append(normalized_data_point)\n",
    "                        break\n",
    "                    else:\n",
    "                        log_pos += 1\n",
    "\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "    return iteration_state_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rt1_5p_datapoints = mine_datapoints(rt1_5p_dir, 'A.RTL1')\n",
    "# rt1_10p_datapoints = mine_datapoints(rt1_10p_dir, 'A.RTL1')\n",
    "# rt1_15p_datapoints = mine_datapoints(rt1_15p_dir, 'A.RTL1')\n",
    "# rt1_20p_datapoints = mine_datapoints(rt1_20p_dir, 'A.RTL1')\n",
    "# rt2_5p_datapoints = mine_datapoints(rt2_5p_dir, 'A.RTL2')\n",
    "# rt2_10p_datapoints = mine_datapoints(rt2_10p_dir, 'A.RTL2')\n",
    "rt2_20p_datapoints = mine_datapoints(rt2_20p_dir, 'A.RTL2')\n",
    "# rt2_15p_datapoints = mine_datapoints(rt2_15p_dir, 'A.RTL2')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Surrogate Model for New State Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lstm to train regression problem\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, lr, input_dims, output_dims) -> None:\n",
    "        super(Model, self).__init__()\n",
    "        self.lr = lr\n",
    "        self.input_dims = input_dims\n",
    "        self.output_dims = output_dims\n",
    "\n",
    "        self.fc1 = nn.Linear(self.input_dims, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, 32)\n",
    "        self.fc4 = nn.Linear(32, self.output_dims)\n",
    "\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr = self.lr)\n",
    "        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n",
    "\n",
    "        self.to(self.device)\n",
    "\n",
    "        self.fc1.weight.data.normal_(0, 1e-1)\n",
    "        self.fc2.weight.data.normal_(0, 1e-1)\n",
    "        self.fc3.weight.data.normal_(0, 1e-2)\n",
    "        self.fc1.bias.data.normal_(0, 1e-1)\n",
    "        self.fc2.bias.data.normal_(0, 1e-1)\n",
    "        self.fc3.bias.data.normal_(0, 1e-2)\n",
    "\n",
    "    def forward(self, state):\n",
    "\n",
    "        fc1_out = nn.functional.sigmoid(self.fc1(state))\n",
    "        fc2_out = nn.functional.sigmoid(self.fc2(fc1_out))\n",
    "        fc3_out = nn.functional.sigmoid(self.fc3(fc2_out))\n",
    "        fc4_out = nn.functional.sigmoid(self.fc4(fc3_out))\n",
    "\n",
    "        return fc4_out\n",
    "\n",
    "    def save_checkpoint(self, chkpt_path):\n",
    "        T.save(self.state_dict(), chkpt_path)\n",
    "\n",
    "    def load_checkpoint(self, chkpt_path):\n",
    "        self.load_state_dict(T.load(chkpt_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StateDataset(Dataset):\n",
    "\n",
    "    def __init__(self, iterations):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.data = []\n",
    "\n",
    "        for iteration in iterations:\n",
    "            self.data += iterations[iteration]\n",
    "\n",
    "        self.data = np.array(self.data)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        states_to_appended = T.tensor(self.data[idx]['this_state'], dtype=T.float)\n",
    "        params_to_append = T.tensor(self.data[idx]['params_set'], dtype=T.float)\n",
    "        \n",
    "        x = T.concat((states_to_appended, params_to_append), dim=0)\n",
    "        # y = T.tensor(self.data[idx]['next_state'], dtype=T.float)\n",
    "        y = T.tensor(self.data[idx]['violation'], dtype=T.float)\n",
    "        return x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rtl1_5p_dataset = StateDataset(rt1_5p_datapoints)\n",
    "# rtl1_10p_dataset = StateDataset(rt1_10p_datapoints)\n",
    "# rtl1_15p_dataset = StateDataset(rt1_15p_datapoints)\n",
    "# rtl1_20p_dataset = StateDataset(rt1_20p_datapoints)\n",
    "# rtl2_5p_dataset = StateDataset(rt2_5p_datapoints)\n",
    "# rtl2_10p_dataset = StateDataset(rt2_10p_datapoints)\n",
    "# rtl2_15p_dataset = StateDataset(rt2_15p_datapoints)\n",
    "rtl2_20p_dataset = StateDataset(rt2_20p_datapoints)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Surrogate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def surrogate_model(dataset, scenario, params):\n",
    "    train_ds, test_ds = T.utils.data.random_split(dataset, (round(len(dataset)*.90),  len(dataset)-round(len(dataset)*.90)))\n",
    "    train_dl = DataLoader(train_ds, batch_size=32, shuffle=True)\n",
    "    test_dl = DataLoader(test_ds, batch_size=32, shuffle=True)\n",
    "\n",
    "    model = Model(\n",
    "        0.0005, \n",
    "        len(env_vars.simulation_vars[scenario]['state_cols']) + len(env_vars.simulation_vars[scenario]['modes_vec']) + params, \n",
    "        1)\n",
    "    mse = nn.MSELoss()\n",
    "\n",
    "    i = 0\n",
    "    print_every = 50\n",
    "    plot_every = 75\n",
    "    loss_history = []\n",
    "    avg_train_loss = []\n",
    "    val_loss_history = []\n",
    "\n",
    "    print(\"Training\", scenario, params, 'params')\n",
    "    for epoch in tqdm(range(50)):\n",
    "        model.optimizer.zero_grad()\n",
    "        for states, new_states in train_dl:\n",
    "            model.optimizer.zero_grad()\n",
    "            pred_states = model(states)\n",
    "            loss = mse(pred_states, new_states)\n",
    "            loss.backward()\n",
    "            model.optimizer.step()\n",
    "            avg_train_loss.append(loss.item())\n",
    "            if i % print_every == 0:\n",
    "                loss_history.append(sum(avg_train_loss)/len(avg_train_loss))\n",
    "                avg_train_loss = []\n",
    "                val_loss = []\n",
    "                for v_state_seqs, v_new_states in test_dl:\n",
    "                    val_pred_states = model(v_state_seqs)\n",
    "                    val_loss.append(mse(val_pred_states, v_new_states).item())\n",
    "                val_loss_history.append(sum(val_loss)/len(val_loss))\n",
    "            i += 1\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot([i for i in range(len(loss_history))], loss_history, label='Training Loss')\n",
    "    plt.plot([i for i in range(len(loss_history))], val_loss_history, label='Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "\n",
    "    model.save_checkpoint(env_vars.MODELS + scenario + '_' + str(params) + 'params')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# surrogate_model(rtl1_5p_dataset, 'A.RTL1', 5)\n",
    "# surrogate_model(rtl1_10p_dataset, 'A.RTL1', 10)\n",
    "# surrogate_model(rtl1_15p_dataset, 'A.RTL1', 15)\n",
    "# surrogate_model(rtl1_20p_dataset, 'A.RTL1', 20)\n",
    "# surrogate_model(rtl2_5p_dataset, 'A.RTL2', 5)\n",
    "# surrogate_model(rtl2_10p_dataset, 'A.RTL2', 10)\n",
    "# surrogate_model(rtl2_15p_dataset, 'A.RTL2', 15)\n",
    "surrogate_model(rtl2_20p_dataset, 'A.RTL2', 20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "493d902229d3d617ba8c4f5d9e81c529b10e1f4a2d98ac3f5bec5523b8518722"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
